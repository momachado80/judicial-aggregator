"""
Router para busca de processos via DJE (Di√°rio de Justi√ßa Eletr√¥nico)
Precis√£o absoluta com filtros avan√ßados
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
import os
from src.scrapers.dje_downloader import baixar_dje_intervalo, obter_cadernos_por_comarca
from src.scrapers.dje_parser import extrair_processos_dje
from src.utils.indexador_dje import indexar_todos_pdfs, ler_cache, filtrar_processos_cache, ordenar_processos
from src.database import SessionLocal
from src.models.processo import Processo
from sqlalchemy.exc import IntegrityError

router = APIRouter(prefix="/dje", tags=["DJE"])

class BuscarDJERequest(BaseModel):
    """
    Request para busca no DJE
    """
    data_inicio: str  # DD/MM/YYYY
    data_fim: str     # DD/MM/YYYY
    comarcas: List[str] = ["S√£o Paulo"]
    tipos_processo: List[str] = ["Invent√°rio", "Div√≥rcio"]
    apenas_imoveis: bool = True
    apenas_ativos: bool = True
    valor_min: Optional[float] = None
    valor_max: Optional[float] = None
    salvar_no_banco: bool = True

class BuscarDJEResponse(BaseModel):
    """Response da busca DJE"""
    total_processos: int
    processos: List[dict]
    pdfs_processados: int
    estatisticas: dict

@router.post("/buscar", response_model=BuscarDJEResponse)
async def buscar_processos_dje(request: BuscarDJERequest):
    """
    Busca processos no DJE com precis√£o absoluta

    Fluxo:
    1. Baixa PDFs do DJE para o intervalo de datas
    2. Faz parsing com filtros precisos (im√≥veis, ativos, comarca, valor)
    3. Retorna apenas processos que atendem TODOS os crit√©rios
    """
    try:
        print("\n" + "="*80)
        print("üîç BUSCA DJE INICIADA")
        print("="*80)
        print(f"üìÖ Per√≠odo: {request.data_inicio} at√© {request.data_fim}")
        print(f"üìç Comarcas: {', '.join(request.comarcas)}")
        print(f"üìã Tipos: {', '.join(request.tipos_processo)}")
        print(f"üè† Apenas im√≥veis: {request.apenas_imoveis}")
        print(f"‚úÖ Apenas ativos: {request.apenas_ativos}")
        if request.valor_min:
            print(f"üí∞ Valor m√≠n: R$ {request.valor_min:,.2f}")
        if request.valor_max:
            print(f"üí∞ Valor m√°x: R$ {request.valor_max:,.2f}")
        print("="*80)

        # PASSO 1: Baixar PDFs
        print("\nüì• PASSO 1: Baixando PDFs do DJE...")
        pdfs = baixar_dje_intervalo(
            data_inicio=request.data_inicio,
            data_fim=request.data_fim,
            comarcas=request.comarcas,
            headless=True
        )

        if not pdfs:
            raise HTTPException(
                status_code=404,
                detail="Nenhum PDF foi baixado. Verifique as datas e comarcas."
            )

        print(f"‚úÖ {len(pdfs)} PDFs baixados")

        # PASSO 2: Processar cada PDF com filtros
        print("\nüîç PASSO 2: Processando PDFs com filtros...")
        todos_processos = []
        estatisticas = {
            "pdfs_processados": 0,
            "processos_encontrados": 0,
            "processos_rejeitados": 0,
            "por_tipo": {},
            "por_relevancia": {},
            "por_comarca": {}
        }

        for pdf_path in pdfs:
            if not os.path.exists(pdf_path):
                print(f"‚ö†Ô∏è  PDF n√£o encontrado: {pdf_path}")
                continue

            print(f"\nüìÑ Processando: {os.path.basename(pdf_path)}")

            processos = extrair_processos_dje(
                pdf_path=pdf_path,
                tipos=request.tipos_processo,
                filtrar_imoveis=request.apenas_imoveis,
                filtrar_ativos=request.apenas_ativos,
                comarcas_filtro=request.comarcas if request.comarcas else None,
                valor_min=request.valor_min,
                valor_max=request.valor_max
            )

            todos_processos.extend(processos)
            estatisticas["pdfs_processados"] += 1

        # PASSO 3: Estat√≠sticas
        print("\nüìä PASSO 3: Gerando estat√≠sticas...")
        estatisticas["processos_encontrados"] = len(todos_processos)

        for p in todos_processos:
            # Por tipo
            tipo = p.get("tipo", "Desconhecido")
            estatisticas["por_tipo"][tipo] = estatisticas["por_tipo"].get(tipo, 0) + 1

            # Por relev√¢ncia
            rel = p.get("relevancia", "Desconhecida")
            estatisticas["por_relevancia"][rel] = estatisticas["por_relevancia"].get(rel, 0) + 1

            # Por comarca
            comarca = p.get("comarca", "Desconhecida")
            estatisticas["por_comarca"][comarca] = estatisticas["por_comarca"].get(comarca, 0) + 1

        # PASSO 4: Salvar no banco (opcional)
        if request.salvar_no_banco and todos_processos:
            print("\nüíæ PASSO 4: Salvando no banco de dados...")
            salvos, duplicados = salvar_processos_dje(todos_processos)
            print(f"‚úÖ {salvos} novos processos salvos")
            print(f"üîÑ {duplicados} duplicados ignorados")
            estatisticas["salvos_bd"] = salvos
            estatisticas["duplicados_bd"] = duplicados

        # Resultado final
        print("\n" + "="*80)
        print("üéâ BUSCA CONCLU√çDA")
        print("="*80)
        print(f"üìä Total de processos: {len(todos_processos)}")
        print(f"üìÑ PDFs processados: {estatisticas['pdfs_processados']}")
        print(f"\nüìã Por tipo:")
        for tipo, count in estatisticas["por_tipo"].items():
            print(f"   {tipo}: {count}")
        print(f"\nüéØ Por relev√¢ncia:")
        for rel, count in estatisticas["por_relevancia"].items():
            print(f"   {rel}: {count}")
        print("="*80)

        return BuscarDJEResponse(
            total_processos=len(todos_processos),
            processos=todos_processos,
            pdfs_processados=estatisticas["pdfs_processados"],
            estatisticas=estatisticas
        )

    except Exception as e:
        print(f"\n‚ùå ERRO: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


def salvar_processos_dje(processos: List[dict]) -> tuple:
    """
    Salva processos do DJE no banco de dados

    Returns:
        (novos, duplicados)
    """
    db = SessionLocal()
    novos = 0
    duplicados = 0

    try:
        for proc_data in processos:
            numero = proc_data.get("numero")
            if not numero:
                continue

            # Verificar se j√° existe
            existe = db.query(Processo).filter(
                Processo.numero_processo == numero
            ).first()

            if existe:
                duplicados += 1
                continue

            # Criar novo processo
            processo = Processo(
                numero_processo=numero,
                tribunal="TJSP",
                tipo_processo=proc_data.get("tipo", "Invent√°rio"),
                classe=proc_data.get("classe", ""),
                comarca=proc_data.get("comarca", ""),
                valor_causa=proc_data.get("valor_causa"),
                relevancia=proc_data.get("relevancia", "M√©dia"),
                score_relevancia=proc_data.get("score_relevancia", 0.5),
                partes=proc_data.get("partes", []),
                status="pendente"
            )

            try:
                db.add(processo)
                db.commit()
                novos += 1
            except IntegrityError:
                db.rollback()
                duplicados += 1

    finally:
        db.close()

    return novos, duplicados


@router.get("/comarcas-disponiveis")
async def listar_comarcas_disponiveis():
    """
    Lista comarcas do TJSP dispon√≠veis para busca

    Retorna:
    - "S√£o Paulo" representa TODOS os foros da capital (26 foros)
    - Comarcas do interior (ex: Piracicaba, Campinas, Santos)

    Total: ~350 comarcas
    """
    from src.utils.comarcas import COMARCAS_TJSP, FOROS_SAO_PAULO_CAPITAL

    # Coletar apenas comarcas do interior (excluir foros da capital)
    comarcas_interior = []

    for codigo, nome in COMARCAS_TJSP.items():
        # Pular foros da capital (ser√£o representados por "S√£o Paulo")
        if codigo in FOROS_SAO_PAULO_CAPITAL:
            continue

        comarcas_interior.append({
            "codigo": codigo,
            "nome": nome,
            "tipo": "interior"
        })

    # Ordenar alfabeticamente
    comarcas_interior.sort(key=lambda x: x["nome"])

    # Adicionar "S√£o Paulo" no in√≠cio (representa todos os foros)
    comarcas_lista = ["S√£o Paulo"] + [c["nome"] for c in comarcas_interior]

    return {
        "comarcas": comarcas_lista,
        "total": len(comarcas_lista),
        "info": "S√£o Paulo representa todos os 26 foros da capital",
        "exemplos": {
            "capital": ["S√£o Paulo"],
            "grande_sp": ["Guarulhos", "Santo Andr√©", "S√£o Bernardo do Campo", "Osasco", "Mogi das Cruzes"],
            "interior": ["Piracicaba", "Campinas", "Santos", "Ribeir√£o Preto", "Sorocaba"]
        }
    }


@router.post("/baixar-pdfs-periodo")
async def baixar_pdfs_periodo(
    background_tasks: BackgroundTasks,
    data_inicio: str,
    data_fim: str,
    comarcas: List[str] = ["S√£o Paulo", "Piracicaba", "Campinas", "Santos"]
):
    """
    üì• Baixa PDFs de um per√≠odo espec√≠fico (ex: 01/01/2024 a 31/01/2024)

    IMPORTANTE: Executa em BACKGROUND. Pode levar HORAS dependendo do per√≠odo!

    Args:
        data_inicio: Data in√≠cio (DD/MM/YYYY ou YYYY-MM-DD)
        data_fim: Data fim (DD/MM/YYYY ou YYYY-MM-DD)
        comarcas: Lista de comarcas para baixar

    Exemplo:
        POST /api/dje/baixar-pdfs-periodo
        {
            "data_inicio": "01/01/2024",
            "data_fim": "31/01/2024",
            "comarcas": ["S√£o Paulo", "Piracicaba"]
        }
    """
    from datetime import datetime

    # Converter formato se vier YYYY-MM-DD
    if "-" in data_inicio:
        data_inicio = datetime.strptime(data_inicio, "%Y-%m-%d").strftime("%d/%m/%Y")
    if "-" in data_fim:
        data_fim = datetime.strptime(data_fim, "%Y-%m-%d").strftime("%d/%m/%Y")

    def baixar_em_background():
        """Fun√ß√£o que roda em background"""
        try:
            print(f"\n{'='*80}")
            print(f"üì• DOWNLOAD DE PDFs POR PER√çODO")
            print(f"üìÖ Per√≠odo: {data_inicio} a {data_fim}")
            print(f"üìç Comarcas: {', '.join(comarcas)}")
            print(f"{'='*80}\n")

            pdfs_baixados = baixar_dje_intervalo(
                data_inicio=data_inicio,
                data_fim=data_fim,
                comarcas=comarcas,
                headless=True
            )

            print(f"\n{'='*80}")
            print(f"‚úÖ DOWNLOAD CONCLU√çDO!")
            print(f"üì¶ Total de PDFs baixados: {len(pdfs_baixados)}")
            print(f"{'='*80}\n")

            # Reindexar automaticamente ap√≥s baixar
            print("üîÑ Reindexando cache...")
            from src.utils.indexador_dje import indexar_todos_pdfs
            cache = indexar_todos_pdfs()
            print(f"‚úÖ Cache atualizado! {cache['total_processos']} processos indexados.")

        except Exception as e:
            print(f"\n‚ùå ERRO no download: {e}\n")
            import traceback
            traceback.print_exc()

    # Adicionar tarefa em background
    background_tasks.add_task(baixar_em_background)

    # Calcular n√∫mero aproximado de dias
    try:
        d1 = datetime.strptime(data_inicio, "%d/%m/%Y")
        d2 = datetime.strptime(data_fim, "%d/%m/%Y")
        dias = (d2 - d1).days
    except:
        dias = "?"

    return {
        "status": "iniciado",
        "mensagem": f"Download de PDFs do per√≠odo {data_inicio} a {data_fim} iniciado em background",
        "periodo": {
            "inicio": data_inicio,
            "fim": data_fim,
            "dias_aproximados": dias
        },
        "comarcas": comarcas,
        "aviso": f"Este processo pode levar v√°rias horas! (~{len(comarcas) * (dias if isinstance(dias, int) else 30)} PDFs)",
        "info": "Ap√≥s o download, o cache ser√° reindexado automaticamente. Acompanhe o progresso nos logs."
    }


@router.post("/baixar-pdfs-automatico")
async def baixar_pdfs_automatico(
    background_tasks: BackgroundTasks,
    dias: int = 30,
    todas_comarcas: bool = True
):
    """
    ‚ö†Ô∏è DEPRECATED: Use /baixar-pdfs-periodo para maior controle

    Baixa PDFs dos √∫ltimos N dias de TODOS os cadernos do TJSP
    """
    from datetime import datetime, timedelta

    # Calcular datas
    data_fim = datetime.now()
    data_inicio = data_fim - timedelta(days=dias)

    data_inicio_str = data_inicio.strftime("%d/%m/%Y")
    data_fim_str = data_fim.strftime("%d/%m/%Y")

    comarcas = ["S√£o Paulo", "Piracicaba", "Campinas", "Santos", "Guarulhos"] if todas_comarcas else ["S√£o Paulo"]

    def baixar_em_background():
        """Fun√ß√£o que roda em background"""
        try:
            print(f"\n{'='*80}")
            print(f"üöÄ INICIANDO DOWNLOAD AUTOM√ÅTICO DE PDFs")
            print(f"üìÖ Per√≠odo: {data_inicio_str} a {data_fim_str} ({dias} dias)")
            print(f"üìç Comarcas: {', '.join(comarcas)}")
            print(f"{'='*80}\n")

            pdfs_baixados = baixar_dje_intervalo(
                data_inicio=data_inicio_str,
                data_fim=data_fim_str,
                comarcas=comarcas,
                headless=True
            )

            print(f"\n{'='*80}")
            print(f"‚úÖ DOWNLOAD CONCLU√çDO!")
            print(f"üì¶ Total de PDFs baixados: {len(pdfs_baixados)}")
            print(f"{'='*80}\n")

        except Exception as e:
            print(f"\n‚ùå ERRO no download em background: {e}\n")

    # Adicionar tarefa em background
    background_tasks.add_task(baixar_em_background)

    return {
        "status": "iniciado",
        "mensagem": f"Download de PDFs dos √∫ltimos {dias} dias foi iniciado em background",
        "periodo": {
            "inicio": data_inicio_str,
            "fim": data_fim_str,
            "dias": dias
        },
        "comarcas": comarcas,
        "info": "O download est√° acontecendo no servidor. Aguarde alguns minutos e verifique os PDFs dispon√≠veis em /api/dje/status"
    }


@router.get("/teste-simples")
async def teste_simples():
    """
    Teste simples: processa 1 PDF sem filtros para verificar se est√° funcionando
    """
    import os
    from src.scrapers.dje_parser import extrair_processos_dje

    pdfs_dir = "data/dje_pdfs"

    if not os.path.exists(pdfs_dir):
        return {"erro": "Diret√≥rio de PDFs n√£o encontrado"}

    # Pegar primeiro PDF dispon√≠vel (caderno 11 - menor)
    pdfs = sorted([
        os.path.join(pdfs_dir, f)
        for f in os.listdir(pdfs_dir)
        if f.endswith('cad11.pdf')
    ])

    if not pdfs:
        return {"erro": "Nenhum PDF encontrado"}

    pdf_teste = pdfs[0]
    pdf_nome = os.path.basename(pdf_teste)

    try:
        # Processar SEM FILTROS
        processos = extrair_processos_dje(
            pdf_path=pdf_teste,
            tipos=["Invent√°rio", "Div√≥rcio"],
            filtrar_imoveis=False,
            filtrar_ativos=False,
            comarcas_filtro=None
        )

        return {
            "sucesso": True,
            "pdf_testado": pdf_nome,
            "total_processos": len(processos),
            "processos": processos[:5],  # Primeiros 5
            "mensagem": f"Teste OK! Encontrados {len(processos)} processos de Invent√°rio/Div√≥rcio"
        }

    except Exception as e:
        return {
            "sucesso": False,
            "erro": str(e),
            "pdf_testado": pdf_nome
        }


@router.get("/status")
async def status_dje():
    """Status do sistema DJE com diagn√≥stico detalhado"""
    import os
    from pathlib import Path

    pdfs_dir = "data/dje_pdfs"
    pdfs_existentes = []
    diagnostico = {}

    # Verificar se est√° em modo Railway (sem Playwright)
    railway_mode = os.getenv("RAILWAY_DEPLOY", "false") == "true"

    # Diagn√≥stico detalhado do filesystem
    try:
        cwd = os.getcwd()
        diagnostico["current_working_directory"] = cwd
        diagnostico["data_dir_exists"] = os.path.exists("data")
        diagnostico["data_dir_absolute_path"] = os.path.abspath("data")
        diagnostico["pdfs_dir_exists"] = os.path.exists(pdfs_dir)
        diagnostico["pdfs_dir_absolute_path"] = os.path.abspath(pdfs_dir)

        # Listar conte√∫do do diret√≥rio data se existir
        if os.path.exists("data"):
            diagnostico["data_dir_contents"] = os.listdir("data")
        else:
            diagnostico["data_dir_contents"] = []

        # Listar PDFs se o diret√≥rio existir
        if os.path.exists(pdfs_dir):
            all_files = os.listdir(pdfs_dir)
            pdfs_existentes = [f for f in all_files if f.endswith('.pdf')]
            diagnostico["all_files_in_pdfs_dir"] = all_files[:20]  # Primeiros 20

            # Verificar tamanho dos arquivos
            if pdfs_existentes:
                sample_pdf = os.path.join(pdfs_dir, pdfs_existentes[0])
                diagnostico["sample_pdf_size_bytes"] = os.path.getsize(sample_pdf)

    except Exception as e:
        diagnostico["error"] = str(e)

    return {
        "status": "online",
        "modo": "railway" if railway_mode else "local",
        "download_disponivel": not railway_mode,
        "pdfs_cache": len(pdfs_existentes),
        "diretorio": pdfs_dir,
        "ultimos_pdfs": sorted(pdfs_existentes, reverse=True)[:10] if pdfs_existentes else [],
        "diagnostico": diagnostico
    }


@router.post("/buscar-cache-instantaneo")
async def buscar_cache_instantaneo(
    tipos_processo: List[str] = ["Invent√°rio", "Div√≥rcio"],
    comarcas: Optional[List[str]] = None,
    apenas_imoveis: bool = False,
    apenas_ativos: bool = True,
    valor_min: Optional[float] = None,
    valor_max: Optional[float] = None,
    data_inicio: Optional[str] = None,
    data_fim: Optional[str] = None,
    ordenar_por: str = "relevancia_desc"
):
    """
    üöÄ BUSCA INSTANT√ÇNEA - Usa cache JSON pr√©-processado

    VELOCIDADE: < 100ms (ao inv√©s de 2+ minutos)

    Este endpoint l√™ um arquivo JSON que cont√©m TODOS os processos
    j√° extra√≠dos dos PDFs. A busca √© EXTREMAMENTE R√ÅPIDA porque
    apenas filtra dados j√° processados.

    IMPORTANTE: O cache precisa ser gerado primeiro com /reindexar

    Args:
        ordenar_por: Crit√©rio de ordena√ß√£o dos resultados
            - "relevancia_desc": Alta relev√¢ncia primeiro (padr√£o)
            - "relevancia_asc": Baixa relev√¢ncia primeiro
            - "data_desc": Mais recente primeiro
            - "data_asc": Mais antigo primeiro
            - "valor_desc": Maior valor primeiro
            - "valor_asc": Menor valor primeiro
    """
    try:
        cache_path = "data/dje_cache.json"

        # Verificar se cache existe
        if not os.path.exists(cache_path):
            raise HTTPException(
                status_code=404,
                detail="Cache n√£o encontrado. Execute /api/dje/reindexar primeiro para gerar o √≠ndice."
            )

        # Ler cache
        cache = ler_cache(cache_path)

        # Filtrar processos (INSTANT√ÇNEO!)
        processos_filtrados = filtrar_processos_cache(
            cache=cache,
            tipos=tipos_processo,
            comarcas=comarcas,
            apenas_imoveis=apenas_imoveis,
            apenas_ativos=apenas_ativos,
            valor_min=valor_min,
            valor_max=valor_max,
            data_inicio=data_inicio,
            data_fim=data_fim
        )

        # Ordenar processos (INSTANT√ÇNEO!)
        processos_filtrados = ordenar_processos(
            processos=processos_filtrados,
            ordenar_por=ordenar_por
        )

        # Estat√≠sticas
        from collections import Counter
        tipos_count = Counter(p.get("tipo") for p in processos_filtrados)
        relevancia_count = Counter(p.get("relevancia") for p in processos_filtrados)

        # Descri√ß√£o da ordena√ß√£o
        ordenacao_desc = {
            "relevancia_desc": "Alta relev√¢ncia primeiro",
            "relevancia_asc": "Baixa relev√¢ncia primeiro",
            "data_desc": "Mais recentes primeiro",
            "data_asc": "Mais antigos primeiro",
            "valor_desc": "Maior valor primeiro",
            "valor_asc": "Menor valor primeiro"
        }

        return {
            "total_processos": len(processos_filtrados),
            "processos": processos_filtrados,
            "pdfs_disponiveis_total": cache["total_pdfs"],
            "pdfs_processados_sucesso": cache["total_pdfs"],
            "data_indexacao": cache["data_indexacao"],
            "ordenacao": {
                "criterio": ordenar_por,
                "descricao": ordenacao_desc.get(ordenar_por, "Sem ordena√ß√£o")
            },
            "estatisticas": {
                "por_tipo": dict(tipos_count),
                "por_relevancia": dict(relevancia_count)
            },
            "mensagem": f"Busca instant√¢nea conclu√≠da! {len(processos_filtrados)} processos encontrados.",
            "cache_info": {
                "total_processos_indexados": cache["total_processos"],
                "total_pdfs_indexados": cache["total_pdfs"]
            }
        }

    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/reindexar")
async def reindexar_pdfs(background_tasks: BackgroundTasks):
    """
    Reindexar TODOS os PDFs e gerar cache JSON

    ATEN√á√ÉO: Este processo leva 10-20 minutos mas s√≥ precisa ser
    executado UMA VEZ ou quando novos PDFs forem adicionados.

    Ap√≥s a indexa√ß√£o, todas as buscas ser√£o INSTANT√ÇNEAS!
    """
    def indexar_background():
        try:
            print("\nüöÄ Iniciando indexa√ß√£o em background...")
            cache = indexar_todos_pdfs()
            print(f"‚úÖ Indexa√ß√£o conclu√≠da! {cache['total_processos']} processos indexados.")
        except Exception as e:
            print(f"‚ùå Erro na indexa√ß√£o: {e}")
            import traceback
            traceback.print_exc()

    background_tasks.add_task(indexar_background)

    return {
        "status": "iniciado",
        "mensagem": "Indexa√ß√£o iniciada em background. Isso levar√° 10-20 minutos.",
        "info": "Acompanhe o progresso nos logs do servidor. Ap√≥s concluir, use /buscar-cache-instantaneo para buscas r√°pidas."
    }


@router.post("/processar-pdfs-cache")
async def processar_pdfs_cache(
    tipos_processo: List[str] = ["Invent√°rio", "Div√≥rcio"],
    comarcas: Optional[List[str]] = None,
    apenas_imoveis: bool = False,
    apenas_ativos: bool = True,
    valor_min: Optional[float] = None,
    valor_max: Optional[float] = None,
    limite_pdfs: int = 1
):
    """
    ‚ö†Ô∏è DEPRECATED: Use /buscar-cache-instantaneo para buscas r√°pidas

    Este endpoint processa PDFs em tempo real (LENTO - 30s por PDF)
    """
    try:
        pdfs_dir = "data/dje_pdfs"

        if not os.path.exists(pdfs_dir):
            raise HTTPException(
                status_code=404,
                detail=f"Diret√≥rio de PDFs n√£o encontrado: {pdfs_dir}"
            )

        # Listar PDFs dispon√≠veis
        todos_pdfs = [
            os.path.join(pdfs_dir, f)
            for f in os.listdir(pdfs_dir)
            if f.endswith('.pdf') and not f.startswith('teste')
        ]

        if not todos_pdfs:
            return {
                "total_processos": 0,
                "processos": [],
                "pdfs_processados": 0,
                "mensagem": "Nenhum PDF encontrado no cache"
            }

        # Ordenar por data (mais recente primeiro) e aplicar limite
        todos_pdfs.sort(reverse=True)
        pdfs_disponiveis = todos_pdfs[:limite_pdfs]

        print(f"\nüìÅ Processando {len(pdfs_disponiveis)} de {len(todos_pdfs)} PDFs dispon√≠veis...")

        # Processar cada PDF
        todos_processos = []
        pdfs_com_erro = []
        pdfs_processados_sucesso = 0

        for pdf_path in pdfs_disponiveis:
            pdf_nome = os.path.basename(pdf_path)
            print(f"\nüìÑ {pdf_nome}")

            try:
                processos = extrair_processos_dje(
                    pdf_path=pdf_path,
                    tipos=tipos_processo,
                    filtrar_imoveis=apenas_imoveis,
                    filtrar_ativos=apenas_ativos,
                    comarcas_filtro=comarcas,
                    valor_min=valor_min,
                    valor_max=valor_max
                )
                todos_processos.extend(processos)
                pdfs_processados_sucesso += 1
                print(f"  ‚úÖ {len(processos)} processos encontrados")

            except Exception as e:
                erro_msg = f"{pdf_nome}: {str(e)}"
                pdfs_com_erro.append(erro_msg)
                print(f"  ‚ö†Ô∏è ERRO ao processar: {str(e)}")
                continue

        # Estat√≠sticas
        from collections import Counter
        tipos_count = Counter(p.get("tipo") for p in todos_processos)
        relevancia_count = Counter(p.get("relevancia") for p in todos_processos)

        return {
            "total_processos": len(todos_processos),
            "processos": todos_processos,
            "pdfs_disponiveis_total": len(todos_pdfs),
            "pdfs_processados": len(pdfs_disponiveis),
            "pdfs_processados_sucesso": pdfs_processados_sucesso,
            "pdfs_com_erro": len(pdfs_com_erro),
            "erros": pdfs_com_erro if pdfs_com_erro else None,
            "estatisticas": {
                "por_tipo": dict(tipos_count),
                "por_relevancia": dict(relevancia_count)
            },
            "mensagem": f"Processados {len(pdfs_disponiveis)} PDFs mais recentes de {len(todos_pdfs)} dispon√≠veis"
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
